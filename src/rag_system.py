"""
RAG (Retrieval-Augmented Generation) ç³»ç»Ÿ
é›†æˆ ModelScopeã€ChromaDBã€DeepSeek API
"""
import os
import sys
import time
import json
import warnings
from typing import List, Dict, Any, Optional
import logging

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings("ignore")
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

try:
    # ModelScopeé•œåƒæºé…ç½®
    os.environ['MODELSCOPE_CACHE'] = '../models'
    from modelscope import snapshot_download
    from modelscope.hub.api import HubApi
    from sentence_transformers import SentenceTransformer
    MODELSCOPE_AVAILABLE = True
    logger.info("âœ… ModelScope available")
except ImportError:
    MODELSCOPE_AVAILABLE = False
    logger.warning("âš ï¸ ModelScope not available")

try:
    import chromadb
    from chromadb.config import Settings
    CHROMADB_AVAILABLE = True
    logger.info("âœ… ChromaDB available")
except ImportError:
    CHROMADB_AVAILABLE = False
    logger.warning("âš ï¸ ChromaDB not available")

try:
    import openai
    OPENAI_AVAILABLE = True
    logger.info("âœ… OpenAI client available")
except ImportError:
    OPENAI_AVAILABLE = False
    logger.warning("âš ï¸ OpenAI client not available")

from utils import load_toutiao_data, clean_text, split_text_by_sentences


class RAGSystem:
    """RAGç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self, config):
        """
        åˆå§‹åŒ–RAGç³»ç»Ÿ
        
        Args:
            config: é…ç½®å¯¹è±¡
        """
        self.config = config
        self.embedding_model = None
        self.chroma_client = None
        self.collection = None
        self.openai_client = None
        self.using_modelscope = False
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.config.MODEL_CACHE_DIR, exist_ok=True)
        os.makedirs(self.config.CHROMA_PERSIST_DIR, exist_ok=True)
        
        logger.info("ğŸš€ RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def initialize(self) -> bool:
        """
        åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
        
        Returns:
            bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        """
        logger.info("ğŸ”§ å¼€å§‹åˆå§‹åŒ–RAGç³»ç»Ÿ...")
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        if not self._initialize_embedding_model():
            logger.error("âŒ åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥")
            return False
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        if not self._initialize_vector_db():
            logger.error("âŒ å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥")
            return False
        
        # åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰
        self._initialize_openai_client()
        
        logger.info("âœ… RAGç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
        return True
    
    def _initialize_embedding_model(self) -> bool:
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        logger.info("ğŸ¤– æ­£åœ¨åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
        
        # ğŸ¯ ä¼˜å…ˆå°è¯•ä½¿ç”¨é…ç½®çš„M3E-Baseæ¨¡å‹
        if self.config.EMBEDDING_MODEL_NAME == "AI-ModelScope/m3e-base":
            try:
                logger.info(f"ğŸš€ ä¼˜å…ˆå°è¯•åŠ è½½M3E-Baseæ¨¡å‹: {self.config.EMBEDDING_MODEL_NAME}")
                
                # å°è¯•ä¸¤ä¸ªå¯èƒ½çš„è·¯å¾„ï¼šé…ç½®è·¯å¾„å’Œå½“å‰ç›®å½•è·¯å¾„
                possible_paths = [
                    os.path.join(self.config.MODEL_CACHE_DIR, "AI-ModelScope", "m3e-base"),  # é…ç½®è·¯å¾„
                    os.path.join("./models", "AI-ModelScope", "m3e-base"),  # å½“å‰ç›®å½•è·¯å¾„
                    os.path.join("models", "AI-ModelScope", "m3e-base")  # ç›¸å¯¹è·¯å¾„
                ]
                
                for local_model_path in possible_paths:
                    if os.path.exists(local_model_path):
                        logger.info(f"ğŸ“ æ‰¾åˆ°æœ¬åœ°æ¨¡å‹è·¯å¾„: {local_model_path}")
                        # ä½¿ç”¨æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹
                        self.embedding_model = SentenceTransformer(local_model_path)
                        logger.info(f"âœ… M3E-Baseæ¨¡å‹åŠ è½½æˆåŠŸ: {local_model_path}")
                        return True
                
                logger.warning("âš ï¸ æœªæ‰¾åˆ°æœ¬åœ°M3E-Baseæ¨¡å‹æ–‡ä»¶")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ M3E-Baseæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        
        # å°è¯•ç›´æ¥åŠ è½½å…¶ä»–é…ç½®çš„åµŒå…¥æ¨¡å‹
        try:
            logger.info(f"ğŸš€ å°è¯•ç›´æ¥åŠ è½½é…ç½®çš„åµŒå…¥æ¨¡å‹: {self.config.EMBEDDING_MODEL_NAME}")
            
            # å°è¯•åŠ è½½é…ç½®çš„åµŒå…¥æ¨¡å‹
            self.embedding_model = SentenceTransformer(
                self.config.EMBEDDING_MODEL_NAME,
                cache_folder=self.config.MODEL_CACHE_DIR
            )
            
            logger.info(f"âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ: {self.config.EMBEDDING_MODEL_NAME}")
            return True
            
        except Exception as e:
            logger.warning(f"âš ï¸ é…ç½®çš„åµŒå…¥æ¨¡å‹ç›´æ¥åŠ è½½å¤±è´¥: {e}")
        
        # å°è¯•ä½¿ç”¨ModelScopeï¼ˆå¦‚æœå¯ç”¨ä¸”ç½‘ç»œæ¡ä»¶å¥½ï¼‰
        if MODELSCOPE_AVAILABLE:
            try:
                logger.info("ğŸš€ å°è¯•ä½¿ç”¨ModelScopeé•œåƒæº...")
                
                # ä¸­æ–‡åµŒå…¥æ¨¡å‹
                model_id = 'iic/nlp_gte_sentence-embedding_chinese-small'
                
                # ä¸‹è½½æ¨¡å‹
                model_dir = snapshot_download(model_id, cache_dir=self.config.MODEL_CACHE_DIR)
                logger.info(f"âœ… æ¨¡å‹ä¸‹è½½æˆåŠŸ: {model_dir}")
                
                # åŠ è½½æ¨¡å‹
                self.embedding_model = SentenceTransformer(model_dir)
                self.using_modelscope = True
                logger.info("âœ… ModelScopeåµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
                return True
                
            except Exception as e:
                logger.warning(f"âš ï¸ ModelScopeåŠ è½½å¤±è´¥: {e}")
        
        logger.error("âŒ æ‰€æœ‰åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥")
        return False
    
    def _initialize_vector_db(self) -> bool:
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        if not CHROMADB_AVAILABLE:
            logger.error("âŒ ChromaDBä¸å¯ç”¨")
            return False
        
        try:
            logger.info("ğŸ—„ï¸ æ­£åœ¨åˆå§‹åŒ–ChromaDB...")
            
            # åˆ›å»ºChromaDBå®¢æˆ·ç«¯
            self.chroma_client = chromadb.PersistentClient(
                path=self.config.CHROMA_PERSIST_DIR,
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True
                )
            )
            
            # è·å–æˆ–åˆ›å»ºé›†åˆ
            try:
                self.collection = self.chroma_client.get_collection(
                    name=self.config.COLLECTION_NAME
                )
                logger.info(f"âœ… å·²è¿æ¥åˆ°ç°æœ‰é›†åˆ: {self.config.COLLECTION_NAME}")
            except:
                self.collection = self.chroma_client.create_collection(
                    name=self.config.COLLECTION_NAME,
                    metadata={"hnsw:space": "cosine"}
                )
                logger.info(f"âœ… åˆ›å»ºæ–°é›†åˆ: {self.config.COLLECTION_NAME}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ChromaDBåˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def _initialize_openai_client(self):
        """åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰"""
        if not OPENAI_AVAILABLE:
            logger.warning("âš ï¸ OpenAIå®¢æˆ·ç«¯ä¸å¯ç”¨")
            return
        
        # å®‰å…¨åœ°æ˜¾ç¤ºAPIå¯†é’¥ï¼ˆåªæ˜¾ç¤ºå‰10ä¸ªå­—ç¬¦ï¼‰
        api_key_display = self.config.DEEPSEEK_API_KEY[:10] + "..." if self.config.DEEPSEEK_API_KEY else "æœªè®¾ç½®"
        logger.info(f"âœ… DeepSeek APIå¯†é’¥: {api_key_display}")
        
        try:
            if self.config.DEEPSEEK_API_KEY and self.config.DEEPSEEK_API_KEY != "sk-YOUR-API-KEY":
                self.openai_client = openai.OpenAI(
                    api_key=self.config.DEEPSEEK_API_KEY,
                    base_url=self.config.DEEPSEEK_BASE_URL
                )
                logger.info("âœ… DeepSeekå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            else:
                logger.warning("âš ï¸ DeepSeek APIå¯†é’¥æœªé…ç½®")
        except Exception as e:
            logger.warning(f"âš ï¸ DeepSeekå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def load_and_index_data(self, data_file: str, max_documents: int = 1000, force_reload: bool = False) -> bool:
        """
        åŠ è½½å’Œç´¢å¼•æ•°æ®
        
        Args:
            data_file: æ•°æ®æ–‡ä»¶è·¯å¾„
            max_documents: æœ€å¤§æ–‡æ¡£æ•°é‡
            force_reload: æ˜¯å¦å¼ºåˆ¶é‡æ–°åŠ è½½
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        logger.info(f"ğŸ“š å¼€å§‹åŠ è½½æ•°æ®: {data_file}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åŠ è½½
        if not force_reload and self.collection:
            try:
                count = self.collection.count()
                if count > 0:
                    logger.info(f"âœ… æ•°æ®å·²å­˜åœ¨ ({count} æ¡æ–‡æ¡£)")
                    return True
            except:
                pass
        
        # æ¸…ç©ºç°æœ‰æ•°æ®ï¼ˆå¦‚æœå¼ºåˆ¶é‡æ–°åŠ è½½ï¼‰
        if force_reload and self.collection:
            try:
                self.chroma_client.delete_collection(self.config.COLLECTION_NAME)
                self.collection = self.chroma_client.create_collection(
                    name=self.config.COLLECTION_NAME,
                    metadata={"hnsw:space": "cosine"}
                )
                logger.info("ğŸ—‘ï¸ å·²æ¸…ç©ºç°æœ‰æ•°æ®")
            except Exception as e:
                logger.warning(f"âš ï¸ æ¸…ç©ºæ•°æ®å¤±è´¥: {e}")
        
        # åŠ è½½åŸå§‹æ•°æ®
        raw_data = load_toutiao_data(data_file, max_documents)
        if not raw_data:
            logger.error("âŒ æ•°æ®åŠ è½½å¤±è´¥")
            return False
        
        logger.info(f"ğŸ“– åŠ è½½äº† {len(raw_data)} æ¡åŸå§‹æ•°æ®")
        
        # å¤„ç†å’Œåˆ†å—æ•°æ®
        all_chunks = []
        all_metadatas = []
        all_ids = []
        
        for i, item in enumerate(raw_data):
            # æ¸…ç†æ–‡æœ¬
            content = clean_text(item.get('content', ''))
            title = clean_text(item.get('title', ''))
            
            if not content:
                continue
            
            # åˆ†å—å¤„ç†
            chunks = split_text_by_sentences(
                content, 
                self.config.MAX_CHUNK_SIZE, 
                self.config.CHUNK_OVERLAP
            )
            
            for j, chunk in enumerate(chunks):
                if len(chunk.strip()) < 20:  # è·³è¿‡å¤ªçŸ­çš„å—
                    continue
                
                chunk_id = f"doc_{i}_chunk_{j}"
                logger.info("âœ… æ–‡æœ¬å—: "+chunk[0:100])
                all_chunks.append(chunk)
                all_metadatas.append({
                    'title': title,
                    'category': item.get('category', ''),
                    'keywords': item.get('keywords', ''),
                    'doc_id': i,
                    'chunk_id': j
                })
                all_ids.append(chunk_id)
        
        if not all_chunks:
            logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬å—")
            return False
        
        logger.info(f"ğŸ“ ç”Ÿæˆäº† {len(all_chunks)} ä¸ªæ–‡æœ¬å—")
        
        # ç´¢å¼•æ•°æ®
        return self._index_chunks(all_chunks, all_metadatas, all_ids)
    
    def _index_chunks(self, chunks: List[str], metadatas: List[Dict], ids: List[str]) -> bool:
        """ç´¢å¼•æ–‡æœ¬å—"""
        try:
            # ä½¿ç”¨åµŒå…¥æ¨¡å‹
            logger.info("ğŸ§® æ­£åœ¨ç”ŸæˆåµŒå…¥å‘é‡...")
            
            # æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡
            batch_size = 32
            embeddings = []
            
            for i in range(0, len(chunks), batch_size):
                batch_chunks = chunks[i:i + batch_size]
                batch_embeddings = self.embedding_model.encode(
                    batch_chunks,
                    convert_to_tensor=False,
                    show_progress_bar=True
                )
                embeddings.extend(batch_embeddings.tolist())
                
                if i % (batch_size * 10) == 0:
                    logger.info(f"ğŸ“Š å·²å¤„ç† {min(i + batch_size, len(chunks))}/{len(chunks)} ä¸ªæ–‡æœ¬å—")
            
            # å­˜å‚¨åˆ°ChromaDB
            logger.info("ğŸ’¾ æ­£åœ¨å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“...")
            self.collection.add(
                embeddings=embeddings,
                documents=chunks,
                metadatas=metadatas,
                ids=ids
            )
            
            logger.info("âœ… å‘é‡ç´¢å¼•å®Œæˆ")
            return True
                
        except Exception as e:
            logger.error(f"âŒ ç´¢å¼•å¤±è´¥: {e}")
            return False
    
    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        æœç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
        
        Returns:
            List[Dict]: æœç´¢ç»“æœ
        """
        try:
            return self._search_embedding(query, top_k)
        except Exception as e:
            logger.error(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []
    
    def _search_embedding(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """ä½¿ç”¨åµŒå…¥å‘é‡æœç´¢"""
        if not self.collection:
            return []
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = self.embedding_model.encode([query])[0].tolist()
        
        # æœç´¢
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k
        )
        
        # æ ¼å¼åŒ–ç»“æœ
        formatted_results = []
        for i in range(len(results['documents'][0])):
            formatted_results.append({
                'content': results['documents'][0][i],
                'score': 1 - results['distances'][0][i],  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
                'metadata': results['metadatas'][0][i] if results['metadatas'][0] else {}
            })
        
        return formatted_results
    
    def generate_answer(self, query: str, context: str) -> str:
        """
        ä½¿ç”¨DeepSeekç”Ÿæˆç­”æ¡ˆ
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
        
        Returns:
            str: ç”Ÿæˆçš„ç­”æ¡ˆ
        """
        if not self.openai_client:
            return f"åŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼š\n{context[:500]}..."
        
        try:
            prompt = f"""
åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚è¯·ç¡®ä¿ç­”æ¡ˆå‡†ç¡®ã€ç®€æ´ä¸”æœ‰ç”¨ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·æ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ— æ³•æ ¹æ®æä¾›çš„ä¿¡æ¯å›ç­”ã€‚
"""
            
            response = self.openai_client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œèƒ½å¤ŸåŸºäºæä¾›çš„ä¿¡æ¯å‡†ç¡®å›ç­”é—®é¢˜ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000,
                temperature=0.1
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"âŒ ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆå›ç­”ã€‚åŸºäºæ£€ç´¢ä¿¡æ¯ï¼š\n{context[:500]}..."
    
    def query(self, question: str, top_k: int = 5) -> Dict[str, Any]:
        """
        å®Œæ•´çš„é—®ç­”æµç¨‹
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            top_k: æ£€ç´¢ç»“æœæ•°é‡
        
        Returns:
            Dict: åŒ…å«ç­”æ¡ˆã€æ¥æºå’Œæ€§èƒ½æŒ‡æ ‡çš„ç»“æœ
        """
        start_time = time.time()
        
        # æœç´¢ç›¸å…³æ–‡æ¡£
        search_start = time.time()
        sources = self.search(question, top_k)
        search_time = time.time() - search_start
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([
            f"ç›¸å…³ä¿¡æ¯ {i+1}ï¼š{source['content']}"
            for i, source in enumerate(sources[:3])  # åªç”¨å‰3ä¸ªç»“æœ
        ])
        logger.info("âœ… ä¸Šä¸‹æ–‡: "+context)
        # ç”Ÿæˆç­”æ¡ˆ
        generate_start = time.time()
        answer = self.generate_answer(question, context)
        generate_time = time.time() - generate_start
        
        total_time = time.time() - start_time
        
        return {
            'answer': answer,
            'sources': sources,
            'search_time': search_time,
            'generate_time': generate_time,
            'total_time': total_time,
            'using_modelscope': self.using_modelscope
        }
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯"""
        try:
            if self.collection:
                return {
                    'total_documents': self.collection.count(),
                    'embedding_model': self.config.EMBEDDING_MODEL_NAME,
                    'using_modelscope': self.using_modelscope,
                    'chunk_size': self.config.MAX_CHUNK_SIZE,
                    'chunk_overlap': self.config.CHUNK_OVERLAP,
                    'collection_name': self.config.COLLECTION_NAME
                }
            else:
                return {'error': 'ç³»ç»Ÿæœªåˆå§‹åŒ–'}
        except Exception as e:
            return {'error': str(e)} 