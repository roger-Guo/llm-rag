"""
RAG (Retrieval-Augmented Generation) ç³»ç»Ÿ
é›†æˆ ModelScopeã€ChromaDBã€DeepSeek API
"""
import os
import sys
import time
import json
import warnings
from typing import List, Dict, Any, Optional
import logging

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings("ignore")
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

try:
    # ModelScopeé•œåƒæºé…ç½®
    os.environ['MODELSCOPE_CACHE'] = '../models'
    from modelscope import snapshot_download
    from modelscope.hub.api import HubApi
    from sentence_transformers import SentenceTransformer
    MODELSCOPE_AVAILABLE = True
    logger.info("âœ… ModelScope available")
except ImportError:
    MODELSCOPE_AVAILABLE = False
    logger.warning("âš ï¸ ModelScope not available")

try:
    import chromadb
    from chromadb.config import Settings
    CHROMADB_AVAILABLE = True
    logger.info("âœ… ChromaDB available")
except ImportError:
    CHROMADB_AVAILABLE = False
    logger.warning("âš ï¸ ChromaDB not available")

try:
    import openai
    OPENAI_AVAILABLE = True
    logger.info("âœ… OpenAI client available")
except ImportError:
    OPENAI_AVAILABLE = False
    logger.warning("âš ï¸ OpenAI client not available")

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    import numpy as np
    SKLEARN_AVAILABLE = True
    logger.info("âœ… Scikit-learn available")
except ImportError:
    SKLEARN_AVAILABLE = False
    logger.warning("âš ï¸ Scikit-learn not available")

from utils import load_toutiao_data, clean_text, split_text_by_sentences


class RAGSystem:
    """RAGç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self, config):
        """
        åˆå§‹åŒ–RAGç³»ç»Ÿ
        
        Args:
            config: é…ç½®å¯¹è±¡
        """
        self.config = config
        self.embedding_model = None
        self.chroma_client = None
        self.collection = None
        self.tfidf_vectorizer = None
        self.tfidf_matrix = None
        self.documents = []
        self.openai_client = None
        self.using_modelscope = False
        self.using_tfidf = False
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.config.MODEL_CACHE_DIR, exist_ok=True)
        os.makedirs(self.config.CHROMA_PERSIST_DIR, exist_ok=True)
        
        logger.info("ğŸš€ RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def initialize(self) -> bool:
        """
        åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
        
        Returns:
            bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        """
        logger.info("ğŸ”§ å¼€å§‹åˆå§‹åŒ–RAGç³»ç»Ÿ...")
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        if not self._initialize_embedding_model():
            logger.error("âŒ åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥")
            return False
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        if not self._initialize_vector_db():
            logger.error("âŒ å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥")
            return False
        
        # åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰
        self._initialize_openai_client()
        
        logger.info("âœ… RAGç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
        return True
    
    def _initialize_embedding_model(self) -> bool:
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        logger.info("ğŸ¤– æ­£åœ¨åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
        
        # ğŸ¯ å¦‚æœé…ç½®ä¸ºTF-IDFä¼˜å…ˆæ¨¡å¼ï¼Œç›´æ¥ä½¿ç”¨TF-IDF
        if hasattr(self.config, 'USE_TFIDF_ONLY') and self.config.USE_TFIDF_ONLY:
            if SKLEARN_AVAILABLE:
                logger.info("ğŸ¯ é…ç½®ä¸ºTF-IDFä¼˜å…ˆæ¨¡å¼ï¼Œç›´æ¥ä½¿ç”¨TF-IDF...")
                self.tfidf_vectorizer = TfidfVectorizer(
                    max_features=5000,
                    ngram_range=(1, 2),
                    stop_words=None
                )
                self.using_tfidf = True
                logger.info("âœ… TF-IDFåˆå§‹åŒ–æˆåŠŸ (ä¼˜å…ˆæ¨¡å¼)")
                return True
            else:
                logger.warning("âš ï¸ TF-IDFä¼˜å…ˆæ¨¡å¼ä½†scikit-learnä¸å¯ç”¨")
        
        # å°è¯•ä½¿ç”¨ModelScopeï¼ˆå¦‚æœå¯ç”¨ä¸”ç½‘ç»œæ¡ä»¶å¥½ï¼‰
        if MODELSCOPE_AVAILABLE:
            try:
                logger.info("ğŸš€ å°è¯•ä½¿ç”¨ModelScopeé•œåƒæº...")
                
                # ä¸­æ–‡åµŒå…¥æ¨¡å‹
                model_id = 'iic/nlp_gte_sentence-embedding_chinese-small'
                
                # ä¸‹è½½æ¨¡å‹
                model_dir = snapshot_download(model_id, cache_dir=self.config.MODEL_CACHE_DIR)
                logger.info(f"âœ… æ¨¡å‹ä¸‹è½½æˆåŠŸ: {model_dir}")
                
                # åŠ è½½æ¨¡å‹
                self.embedding_model = SentenceTransformer(model_dir)
                self.using_modelscope = True
                logger.info("âœ… ModelScopeåµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
                return True
                
            except Exception as e:
                logger.warning(f"âš ï¸ ModelScopeåŠ è½½å¤±è´¥: {e}")
        
        # å°è¯•ä½¿ç”¨å°å‹æœ¬åœ°æ¨¡å‹ï¼ˆæŒ‰å¤§å°ä¼˜å…ˆçº§ï¼‰
        models_to_try = [
            self.config.EMBEDDING_MODEL_NAME,  # ä¸»æ¨¡å‹
            *self.config.ALTERNATIVE_MODELS    # å¤‡é€‰æ¨¡å‹
        ]
        
        for model_name in models_to_try:
            try:
                logger.info(f"ğŸ”„ å°è¯•åŠ è½½å°å‹æ¨¡å‹: {model_name}")
                
                # ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹
                self.embedding_model = SentenceTransformer(
                    model_name,
                    cache_folder=self.config.MODEL_CACHE_DIR
                )
                
                # æ£€æŸ¥æ¨¡å‹å¤§å°ï¼ˆå¯é€‰ï¼‰
                model_size = self._estimate_model_size(model_name)
                logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name} (é¢„ä¼°å¤§å°: {model_size})")
                return True
                
            except Exception as e:
                logger.warning(f"âš ï¸ æ¨¡å‹ {model_name} åŠ è½½å¤±è´¥: {e}")
                continue
        
        # ä½¿ç”¨TF-IDFå¤‡é€‰æ–¹æ¡ˆ
        if SKLEARN_AVAILABLE:
            logger.info("ğŸ”„ æ‰€æœ‰åµŒå…¥æ¨¡å‹éƒ½å¤±è´¥ï¼Œä½¿ç”¨TF-IDFä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ...")
            self.tfidf_vectorizer = TfidfVectorizer(
                max_features=5000,
                ngram_range=(1, 2),
                stop_words=None
            )
            self.using_tfidf = True
            logger.info("âœ… TF-IDFåˆå§‹åŒ–æˆåŠŸ (å¤‡é€‰æ–¹æ¡ˆ)")
            return True
        
        logger.error("âŒ æ‰€æœ‰åµŒå…¥æ–¹æ³•éƒ½å¤±è´¥äº†")
        return False
    
    def _estimate_model_size(self, model_name: str) -> str:
        """ä¼°ç®—æ¨¡å‹å¤§å°"""
        size_map = {
            "sentence-transformers/paraphrase-MiniLM-L3-v2": "17MB",
            "sentence-transformers/all-MiniLM-L6-v2": "22MB", 
            "sentence-transformers/paraphrase-MiniLM-L6-v2": "22MB",
            "sentence-transformers/all-MiniLM-L12-v2": "33MB"
        }
        return size_map.get(model_name, "æœªçŸ¥å¤§å°")
    
    def _initialize_vector_db(self) -> bool:
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        if not CHROMADB_AVAILABLE:
            logger.error("âŒ ChromaDBä¸å¯ç”¨")
            return False
        
        try:
            logger.info("ğŸ—„ï¸ æ­£åœ¨åˆå§‹åŒ–ChromaDB...")
            
            # åˆ›å»ºChromaDBå®¢æˆ·ç«¯
            self.chroma_client = chromadb.PersistentClient(
                path=self.config.CHROMA_PERSIST_DIR,
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True
                )
            )
            
            # è·å–æˆ–åˆ›å»ºé›†åˆ
            try:
                self.collection = self.chroma_client.get_collection(
                    name=self.config.COLLECTION_NAME
                )
                logger.info(f"âœ… å·²è¿æ¥åˆ°ç°æœ‰é›†åˆ: {self.config.COLLECTION_NAME}")
            except:
                # å¦‚æœä½¿ç”¨TF-IDFï¼Œä¸éœ€è¦metadata
                metadata_config = None if self.using_tfidf else {"hnsw:space": "cosine"}
                
                self.collection = self.chroma_client.create_collection(
                    name=self.config.COLLECTION_NAME,
                    metadata=metadata_config
                )
                logger.info(f"âœ… åˆ›å»ºæ–°é›†åˆ: {self.config.COLLECTION_NAME}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ChromaDBåˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def _initialize_openai_client(self):
        """åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰"""
        if not OPENAI_AVAILABLE:
            logger.warning("âš ï¸ OpenAIå®¢æˆ·ç«¯ä¸å¯ç”¨")
            return
        
        # å®‰å…¨åœ°æ˜¾ç¤ºAPIå¯†é’¥ï¼ˆåªæ˜¾ç¤ºå‰10ä¸ªå­—ç¬¦ï¼‰
        api_key_display = self.config.DEEPSEEK_API_KEY[:10] + "..." if self.config.DEEPSEEK_API_KEY else "æœªè®¾ç½®"
        logger.info(f"âœ… DeepSeek APIå¯†é’¥: {api_key_display}")
        
        try:
            if self.config.DEEPSEEK_API_KEY and self.config.DEEPSEEK_API_KEY != "sk-YOUR-API-KEY":
                self.openai_client = openai.OpenAI(
                    api_key=self.config.DEEPSEEK_API_KEY,
                    base_url=self.config.DEEPSEEK_BASE_URL
                )
                logger.info("âœ… DeepSeekå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            else:
                logger.warning("âš ï¸ DeepSeek APIå¯†é’¥æœªé…ç½®")
        except Exception as e:
            logger.warning(f"âš ï¸ DeepSeekå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def load_and_index_data(self, data_file: str, max_documents: int = 1000, force_reload: bool = False) -> bool:
        """
        åŠ è½½å’Œç´¢å¼•æ•°æ®
        
        Args:
            data_file: æ•°æ®æ–‡ä»¶è·¯å¾„
            max_documents: æœ€å¤§æ–‡æ¡£æ•°é‡
            force_reload: æ˜¯å¦å¼ºåˆ¶é‡æ–°åŠ è½½
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        logger.info(f"ğŸ“š å¼€å§‹åŠ è½½æ•°æ®: {data_file}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åŠ è½½
        if not force_reload and self.collection:
            try:
                count = self.collection.count()
                if count > 0:
                    logger.info(f"âœ… æ•°æ®å·²å­˜åœ¨ ({count} æ¡æ–‡æ¡£)")
                    return True
            except:
                pass
        
        # æ¸…ç©ºç°æœ‰æ•°æ®ï¼ˆå¦‚æœå¼ºåˆ¶é‡æ–°åŠ è½½ï¼‰
        if force_reload and self.collection:
            try:
                self.chroma_client.delete_collection(self.config.COLLECTION_NAME)
                self.collection = self.chroma_client.create_collection(
                    name=self.config.COLLECTION_NAME,
                    metadata=None if self.using_tfidf else {"hnsw:space": "cosine"}
                )
                logger.info("ğŸ—‘ï¸ å·²æ¸…ç©ºç°æœ‰æ•°æ®")
            except Exception as e:
                logger.warning(f"âš ï¸ æ¸…ç©ºæ•°æ®å¤±è´¥: {e}")
        
        # åŠ è½½åŸå§‹æ•°æ®
        raw_data = load_toutiao_data(data_file, max_documents)
        if not raw_data:
            logger.error("âŒ æ•°æ®åŠ è½½å¤±è´¥")
            return False
        
        logger.info(f"ğŸ“– åŠ è½½äº† {len(raw_data)} æ¡åŸå§‹æ•°æ®")
        
        # å¤„ç†å’Œåˆ†å—æ•°æ®
        all_chunks = []
        all_metadatas = []
        all_ids = []
        
        for i, item in enumerate(raw_data):
            # æ¸…ç†æ–‡æœ¬
            content = clean_text(item.get('content', ''))
            title = clean_text(item.get('title', ''))
            
            if not content:
                continue
            
            # åˆ†å—å¤„ç†
            chunks = split_text_by_sentences(
                content, 
                self.config.MAX_CHUNK_SIZE, 
                self.config.CHUNK_OVERLAP
            )
            
            for j, chunk in enumerate(chunks):
                if len(chunk.strip()) < 20:  # è·³è¿‡å¤ªçŸ­çš„å—
                    continue
                
                chunk_id = f"doc_{i}_chunk_{j}"
                all_chunks.append(chunk)
                all_metadatas.append({
                    'title': title,
                    'category': item.get('category', ''),
                    'keywords': item.get('keywords', ''),
                    'doc_id': i,
                    'chunk_id': j
                })
                all_ids.append(chunk_id)
        
        if not all_chunks:
            logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬å—")
            return False
        
        logger.info(f"ğŸ“ ç”Ÿæˆäº† {len(all_chunks)} ä¸ªæ–‡æœ¬å—")
        
        # ç´¢å¼•æ•°æ®
        return self._index_chunks(all_chunks, all_metadatas, all_ids)
    
    def _index_chunks(self, chunks: List[str], metadatas: List[Dict], ids: List[str]) -> bool:
        """ç´¢å¼•æ–‡æœ¬å—"""
        try:
            if self.using_tfidf:
                # ä½¿ç”¨TF-IDF
                logger.info("ğŸ” ä½¿ç”¨TF-IDFè¿›è¡Œç´¢å¼•...")
                self.documents = chunks
                self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(chunks)
                logger.info("âœ… TF-IDFç´¢å¼•å®Œæˆ")
                return True
            else:
                # ä½¿ç”¨åµŒå…¥æ¨¡å‹
                logger.info("ğŸ§® æ­£åœ¨ç”ŸæˆåµŒå…¥å‘é‡...")
                
                # æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡
                batch_size = 32
                embeddings = []
                
                for i in range(0, len(chunks), batch_size):
                    batch_chunks = chunks[i:i + batch_size]
                    batch_embeddings = self.embedding_model.encode(
                        batch_chunks,
                        convert_to_tensor=False,
                        show_progress_bar=True
                    )
                    embeddings.extend(batch_embeddings.tolist())
                    
                    if i % (batch_size * 10) == 0:
                        logger.info(f"ğŸ“Š å·²å¤„ç† {min(i + batch_size, len(chunks))}/{len(chunks)} ä¸ªæ–‡æœ¬å—")
                
                # å­˜å‚¨åˆ°ChromaDB
                logger.info("ğŸ’¾ æ­£åœ¨å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“...")
                self.collection.add(
                    embeddings=embeddings,
                    documents=chunks,
                    metadatas=metadatas,
                    ids=ids
                )
                
                logger.info("âœ… å‘é‡ç´¢å¼•å®Œæˆ")
                return True
                
        except Exception as e:
            logger.error(f"âŒ ç´¢å¼•å¤±è´¥: {e}")
            return False
    
    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        æœç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
        
        Returns:
            List[Dict]: æœç´¢ç»“æœ
        """
        try:
            if self.using_tfidf:
                return self._search_tfidf(query, top_k)
            else:
                return self._search_embedding(query, top_k)
        except Exception as e:
            logger.error(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []
    
    def _search_tfidf(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """ä½¿ç”¨TF-IDFæœç´¢"""
        if self.tfidf_matrix is None:
            return []
        
        # è½¬æ¢æŸ¥è¯¢
        query_vector = self.tfidf_vectorizer.transform([query])
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()
        
        # è·å–top_kç»“æœ
        top_indices = similarities.argsort()[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0:
                results.append({
                    'content': self.documents[idx],
                    'score': float(similarities[idx]),
                    'metadata': {'method': 'TF-IDF'}
                })
        
        return results
    
    def _search_embedding(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """ä½¿ç”¨åµŒå…¥å‘é‡æœç´¢"""
        if not self.collection:
            return []
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = self.embedding_model.encode([query])[0].tolist()
        
        # æœç´¢
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k
        )
        
        # æ ¼å¼åŒ–ç»“æœ
        formatted_results = []
        for i in range(len(results['documents'][0])):
            formatted_results.append({
                'content': results['documents'][0][i],
                'score': 1 - results['distances'][0][i],  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
                'metadata': results['metadatas'][0][i] if results['metadatas'][0] else {}
            })
        
        return formatted_results
    
    def generate_answer(self, query: str, context: str) -> str:
        """
        ä½¿ç”¨DeepSeekç”Ÿæˆç­”æ¡ˆ
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
        
        Returns:
            str: ç”Ÿæˆçš„ç­”æ¡ˆ
        """
        if not self.openai_client:
            return f"åŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼š\n{context[:500]}..."
        
        try:
            prompt = f"""
åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚è¯·ç¡®ä¿ç­”æ¡ˆå‡†ç¡®ã€ç®€æ´ä¸”æœ‰ç”¨ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·æ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ— æ³•æ ¹æ®æä¾›çš„ä¿¡æ¯å›ç­”ã€‚
"""
            
            response = self.openai_client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œèƒ½å¤ŸåŸºäºæä¾›çš„ä¿¡æ¯å‡†ç¡®å›ç­”é—®é¢˜ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000,
                temperature=0.1
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"âŒ ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆå›ç­”ã€‚åŸºäºæ£€ç´¢ä¿¡æ¯ï¼š\n{context[:500]}..."
    
    def query(self, question: str, top_k: int = 5) -> Dict[str, Any]:
        """
        å®Œæ•´çš„é—®ç­”æµç¨‹
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            top_k: æ£€ç´¢ç»“æœæ•°é‡
        
        Returns:
            Dict: åŒ…å«ç­”æ¡ˆã€æ¥æºå’Œæ€§èƒ½æŒ‡æ ‡çš„ç»“æœ
        """
        start_time = time.time()
        
        # æœç´¢ç›¸å…³æ–‡æ¡£
        search_start = time.time()
        sources = self.search(question, top_k)
        search_time = time.time() - search_start
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([
            f"ç›¸å…³ä¿¡æ¯ {i+1}ï¼š{source['content']}"
            for i, source in enumerate(sources[:3])  # åªç”¨å‰3ä¸ªç»“æœ
        ])
        logger.info("âœ… ä¸Šä¸‹æ–‡: "+context)
        # ç”Ÿæˆç­”æ¡ˆ
        generate_start = time.time()
        answer = self.generate_answer(question, context)
        generate_time = time.time() - generate_start
        
        total_time = time.time() - start_time
        
        return {
            'answer': answer,
            'sources': sources,
            'search_time': search_time,
            'generate_time': generate_time,
            'total_time': total_time,
            'using_modelscope': self.using_modelscope,
            'using_tfidf': self.using_tfidf
        }
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯"""
        try:
            if self.using_tfidf:
                return {
                    'total_documents': len(self.documents) if self.documents else 0,
                    'embedding_model': 'TF-IDF',
                    'using_modelscope': False,
                    'using_tfidf': True,
                    'chunk_size': self.config.MAX_CHUNK_SIZE,
                    'chunk_overlap': self.config.CHUNK_OVERLAP,
                    'collection_name': self.config.COLLECTION_NAME
                }
            elif self.collection:
                return {
                    'total_documents': self.collection.count(),
                    'embedding_model': self.config.EMBEDDING_MODEL_NAME,
                    'using_modelscope': self.using_modelscope,
                    'using_tfidf': False,
                    'chunk_size': self.config.MAX_CHUNK_SIZE,
                    'chunk_overlap': self.config.CHUNK_OVERLAP,
                    'collection_name': self.config.COLLECTION_NAME
                }
            else:
                return {'error': 'ç³»ç»Ÿæœªåˆå§‹åŒ–'}
        except Exception as e:
            return {'error': str(e)} 